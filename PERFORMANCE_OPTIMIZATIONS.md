# Performance Optimizations for Quiz Service

## T·ªïng quan c√°c t·ªëi ∆∞u ƒë√£ th·ª±c hi·ªán

### 1. ‚ö° Embedding Model Loading (Ti·∫øt ki·ªám ~3-5 gi√¢y m·ªói request)

**V·∫•n ƒë·ªÅ g·ªëc:** `SentenceTransformer` ƒë∆∞·ª£c kh·ªüi t·∫°o m·ªõi m·ªói l·∫ßn x·ª≠ l√Ω document, t·ªën ~3-5 gi√¢y cho m·ªói l·∫ßn load.

**Gi·∫£i ph√°p:**
- Singleton pattern v·ªõi global cache cho embedding model
- Pre-load model khi server kh·ªüi ƒë·ªông (background task)
- Model ƒë∆∞·ª£c t√°i s·ª≠ d·ª•ng cho t·∫•t c·∫£ requests

```python
# Tr∆∞·ªõc
embedding_model = SentenceTransformer(self.settings.EMBEDDING_MODEL)  # Load m·ªói request

# Sau
@property
def embedding_model(self):
    """Lazy-load and cache embedding model."""
    if self._embedding_model is None:
        self._embedding_model = _load_embedding_model(...)
    return self._embedding_model
```

### 2. üñ•Ô∏è GPU/Device Optimization

**C·∫£i ti·∫øn:**
- T·ª± ƒë·ªông detect v√† s·ª≠ d·ª•ng GPU (CUDA) n·∫øu c√≥
- H·ªó tr·ª£ Apple Silicon (MPS)
- Fallback v·ªÅ CPU n·∫øu kh√¥ng c√≥ GPU

```python
def _get_device() -> str:
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"  # Apple Silicon
    return "cpu"
```

### 3. üîÑ Async Encoding (Non-blocking)

**V·∫•n ƒë·ªÅ g·ªëc:** Encoding embeddings block event loop, l√†m ch·∫≠m to√†n b·ªô service.

**Gi·∫£i ph√°p:**
- S·ª≠ d·ª•ng `ThreadPoolExecutor` ƒë·ªÉ ch·∫°y encoding trong background thread
- Event loop kh√¥ng b·ªã block, c√≥ th·ªÉ x·ª≠ l√Ω requests kh√°c

```python
async def _encode_texts_async(self, texts: List[str], batch_size: int = 64) -> np.ndarray:
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(self._executor, _encode)
```

### 4. üì¶ Optimized Batch Processing

**C·∫£i ti·∫øn:**
- Dynamic batch size d·ª±a tr√™n device (128 cho GPU, 64 cho CPU)
- Pre-normalize embeddings trong encode step, skip trong FAISS add
- S·ª≠ d·ª•ng dict comprehension thay v√¨ loops cho metadata

```python
# Batch size t·ªëi ∆∞u theo device
batch_size = 128 if self._device == "cuda" else 64

# Skip double normalization
self.faiss_index.add_embeddings_batch(chunk_ids, embeddings, metadata_list, already_normalized=True)
```

### 5. üóÉÔ∏è FAISS Index Optimization

**C·∫£i ti·∫øn:**
- Skip normalization n·∫øu embeddings ƒë√£ ƒë∆∞·ª£c normalize
- Batch update metadata v·ªõi dict.update() thay v√¨ loop
- Gi·∫£m memory copies kh√¥ng c·∫ßn thi·∫øt

```python
def add_embeddings_batch(self, ..., already_normalized: bool = False):
    if not already_normalized:
        faiss.normalize_L2(embeddings)  # Skip n·∫øu ƒë√£ normalize
    
    # Batch update thay v√¨ loop
    new_id_map = {start_id + i: chunk_id for i, chunk_id in enumerate(chunk_ids)}
    self.id_map.update(new_id_map)
```

### 6. üöÄ Server Startup Preloading

**C·∫£i ti·∫øn:**
- Pre-load embedding model khi server kh·ªüi ƒë·ªông
- Background task ƒë·ªÉ kh√¥ng block server startup
- First request kh√¥ng c·∫ßn ch·ªù model loading

```python
async def preload_embedding_model():
    """Pre-load embedding model in background to avoid cold start latency."""
    from app.services.quiz_service import _load_embedding_model, _get_device
    _load_embedding_model(settings.EMBEDDING_MODEL, _get_device())

# Trong lifespan
asyncio.create_task(preload_embedding_model())
```

## üìä K·∫øt qu·∫£ ∆∞·ªõc t√≠nh

| Optimization | Ti·∫øt ki·ªám th·ªùi gian |
|--------------|---------------------|
| Model caching | ~3-5 gi√¢y/request |
| GPU utilization | 2-5x faster encoding |
| Async encoding | Non-blocking IO |
| Skip normalization | ~10-20% faster FAISS add |
| Server preload | Eliminate first-request latency |

## üß™ C√°ch test performance

```bash
# Ch·∫°y benchmark
python scripts/benchmark_embeddings.py

# V·ªõi file c·ª• th·ªÉ
python scripts/benchmark_embeddings.py --file path/to/document.pdf

# V·ªõi sample text
python scripts/benchmark_embeddings.py --text "Sample text" --count 100
```

## üìù Config t·ªëi ∆∞u th√™m

Trong `.env`:

```env
# TƒÉng batch size n·∫øu c√≥ nhi·ªÅu VRAM
EMBEDDING_BATCH_SIZE=128

# Model nh·ªè h∆°n cho t·ªëc ƒë·ªô (trade-off accuracy)
# EMBEDDING_MODEL=all-MiniLM-L6-v2  # 384 dim, faster
EMBEDDING_MODEL=all-mpnet-base-v2   # 768 dim, more accurate
```

## üîß T·ªëi ∆∞u th√™m c√≥ th·ªÉ th·ª±c hi·ªán

1. **ONNX Runtime**: Convert model sang ONNX ƒë·ªÉ inference nhanh h∆°n
2. **Quantization**: INT8 quantization cho model nh·ªè h∆°n, nhanh h∆°n
3. **Redis Caching**: Cache embeddings trong Redis cho multiple workers
4. **Parallel Chunking**: Song song h√≥a b∆∞·ªõc chunking
5. **Streaming Processing**: Process chunks theo batches thay v√¨ t·∫•t c·∫£ c√πng l√∫c
