"""
Query Relevance Detector - Kiá»ƒm tra Ä‘á»™ liÃªn quan cá»§a query vá»›i document.

PhÃ¡t hiá»‡n khi query cá»§a ngÆ°á»i dÃ¹ng khÃ´ng liÃªn quan Ä‘áº¿n tÃ i liá»‡u
Ä‘á»ƒ trÃ¡nh sinh cÃ¢u há»i tá»« cÃ¡c chunks khÃ´ng phÃ¹ há»£p.
"""

import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from loguru import logger


@dataclass
class RelevanceResult:
    """Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ Ä‘á»™ liÃªn quan"""
    is_relevant: bool  # Query cÃ³ liÃªn quan khÃ´ng?
    confidence: float  # Äá»™ tin cáº­y (0-1)
    relevance_score: float  # Äiá»ƒm liÃªn quan tá»•ng há»£p (0-1)
    strategy: str  # Chiáº¿n lÆ°á»£c Ä‘á» xuáº¥t: "search", "hybrid", "representative"
    warning_message: Optional[str]  # Cáº£nh bÃ¡o cho ngÆ°á»i dÃ¹ng (náº¿u cÃ³)
    details: Dict[str, Any]  # Chi tiáº¿t phÃ¢n tÃ­ch


class QueryRelevanceDetector:
    """
    PhÃ¡t hiá»‡n Ä‘á»™ liÃªn quan giá»¯a query vÃ  document content.
    
    Sá»­ dá»¥ng nhiá»u phÆ°Æ¡ng phÃ¡p:
    1. Semantic similarity vá»›i top chunks
    2. Semantic similarity vá»›i document overview
    3. Token overlap analysis
    4. Score distribution analysis
    """
    
    def __init__(
        self,
        high_relevance_threshold: float = 0.65,
        low_relevance_threshold: float = 0.35,
        min_top_score: float = 0.4,
        max_score_variance: float = 0.15
    ):
        """
        Args:
            high_relevance_threshold: NgÆ°á»¡ng Ä‘á»ƒ xem lÃ  "highly relevant" (>= 0.65)
            low_relevance_threshold: NgÆ°á»¡ng Ä‘á»ƒ xem lÃ  "low relevant" (< 0.35)
            min_top_score: Äiá»ƒm tá»‘i thiá»ƒu cá»§a chunk top 1 Ä‘á»ƒ xem lÃ  relevant
            max_score_variance: PhÆ°Æ¡ng sai tá»‘i Ä‘a cá»§a scores (náº¿u cao = khÃ´ng rÃµ rÃ ng)
        """
        self.high_relevance_threshold = high_relevance_threshold
        self.low_relevance_threshold = low_relevance_threshold
        self.min_top_score = min_top_score
        self.max_score_variance = max_score_variance
    
    def analyze_query_relevance(
        self,
        query: str,
        query_embedding: np.ndarray,
        search_results: List[Tuple[str, float, Dict]],
        document_overview: Optional[str] = None,
        chunks: Optional[List[Any]] = None,
        embeddings: Optional[np.ndarray] = None,
        embedding_model = None
    ) -> RelevanceResult:
        """
        PhÃ¢n tÃ­ch Ä‘á»™ liÃªn quan cá»§a query vá»›i document.
        
        Args:
            query: Query string tá»« ngÆ°á»i dÃ¹ng
            query_embedding: Embedding cá»§a query
            search_results: Káº¿t quáº£ hybrid search [(chunk_id, score, metadata), ...]
            document_overview: TÃ³m táº¯t/overview cá»§a document (optional)
            chunks: List of all chunks (optional, for deeper analysis)
            embeddings: Embeddings cá»§a táº¥t cáº£ chunks (optional)
            embedding_model: Model Ä‘á»ƒ encode text (optional)
        
        Returns:
            RelevanceResult vá»›i phÃ¢n tÃ­ch chi tiáº¿t
        """
        logger.info(f"ðŸ” Analyzing query relevance: '{query[:50]}...'")
        
        details = {}
        relevance_scores = []
        
        # === 1. PhÃ¢n tÃ­ch Top Search Scores ===
        if search_results and len(search_results) > 0:
            top_scores = [score for _, score, _ in search_results[:5]]
            
            # Score cá»§a chunk tá»‘t nháº¥t
            top_score = top_scores[0]
            details["top_score"] = float(top_score)
            
            # PhÆ°Æ¡ng sai cá»§a top 5 scores (náº¿u cao = khÃ´ng cÃ³ chunk nÃ o ná»•i trá»™i)
            if len(top_scores) > 1:
                score_variance = float(np.var(top_scores))
                details["score_variance"] = score_variance
            else:
                score_variance = 0
            
            # ÄÃ¡nh giÃ¡: top_score cÃ ng cao cÃ ng tá»‘t, variance cÃ ng tháº¥p cÃ ng rÃµ rÃ ng
            score_relevance = top_score
            
            # Penalty náº¿u variance quÃ¡ cao (khÃ´ng cÃ³ chunk nÃ o ná»•i báº­t)
            if score_variance > self.max_score_variance:
                score_relevance *= 0.8
                details["high_variance_penalty"] = True
            
            relevance_scores.append(("top_score", score_relevance, 0.4))  # weight 0.4
        
        # === 2. Semantic Similarity vá»›i Document Overview ===
        if document_overview and embedding_model:
            try:
                overview_embedding = embedding_model.encode([document_overview], convert_to_numpy=True)[0]
                
                # Normalize
                query_norm = query_embedding / (np.linalg.norm(query_embedding) + 1e-9)
                overview_norm = overview_embedding / (np.linalg.norm(overview_embedding) + 1e-9)
                
                # Cosine similarity
                overview_similarity = float(np.dot(query_norm, overview_norm))
                details["overview_similarity"] = overview_similarity
                
                relevance_scores.append(("overview", overview_similarity, 0.3))  # weight 0.3
                
            except Exception as e:
                logger.warning(f"Could not compute overview similarity: {e}")
        
        # === 3. Token Overlap Analysis ===
        token_overlap = self._compute_token_overlap(query, search_results)
        details["token_overlap"] = token_overlap
        relevance_scores.append(("token_overlap", token_overlap, 0.15))  # weight 0.15
        
        # === 4. Score Distribution Analysis ===
        if search_results and len(search_results) >= 5:
            # Kiá»ƒm tra xem cÃ³ gap lá»›n giá»¯a top chunks vÃ  cÃ¡c chunks khÃ¡c khÃ´ng
            scores = [score for _, score, _ in search_results[:10]]
            score_drop = scores[0] - np.mean(scores[1:5]) if len(scores) > 1 else 0
            
            # Gap lá»›n = cÃ³ chunk rÃµ rÃ ng liÃªn quan nháº¥t
            distribution_score = min(1.0, score_drop * 3)  # Normalize
            details["score_drop"] = float(score_drop)
            details["distribution_score"] = distribution_score
            
            relevance_scores.append(("distribution", distribution_score, 0.15))  # weight 0.15
        
        # === TÃ­nh toÃ¡n Relevance Score tá»•ng há»£p ===
        if relevance_scores:
            # Weighted average
            total_weight = sum(w for _, _, w in relevance_scores)
            weighted_sum = sum(score * w for _, score, w in relevance_scores)
            final_relevance = weighted_sum / total_weight if total_weight > 0 else 0
        else:
            final_relevance = 0.5  # Default neutral score
        
        details["component_scores"] = {
            name: float(score) for name, score, _ in relevance_scores
        }
        details["final_relevance"] = float(final_relevance)
        
        # === Quyáº¿t Ä‘á»‹nh Strategy vÃ  Warning ===
        is_relevant, strategy, warning, confidence = self._determine_strategy(
            final_relevance, details
        )
        
        logger.info(
            f"âœ“ Relevance analysis: score={final_relevance:.3f}, "
            f"strategy={strategy}, relevant={is_relevant}"
        )
        
        return RelevanceResult(
            is_relevant=is_relevant,
            confidence=confidence,
            relevance_score=final_relevance,
            strategy=strategy,
            warning_message=warning,
            details=details
        )
    
    def _compute_token_overlap(
        self,
        query: str,
        search_results: List[Tuple[str, float, Dict]]
    ) -> float:
        """TÃ­nh token overlap giá»¯a query vÃ  top chunks."""
        if not search_results:
            return 0.0
        
        # Tokenize query (simple word-based)
        query_tokens = set(query.lower().split())
        
        # Get text from top 3 results
        top_texts = []
        for _, _, meta in search_results[:3]:
            text = meta.get("text", "")
            if text:
                top_texts.append(text.lower())
        
        if not top_texts:
            return 0.0
        
        # Compute overlap
        combined_text = " ".join(top_texts)
        chunk_tokens = set(combined_text.split())
        
        if not query_tokens:
            return 0.0
        
        overlap = len(query_tokens.intersection(chunk_tokens))
        overlap_ratio = overlap / len(query_tokens)
        
        return min(1.0, overlap_ratio)
    
    def _determine_strategy(
        self,
        relevance_score: float,
        details: Dict[str, Any]
    ) -> Tuple[bool, str, Optional[str], float]:
        """
        Quyáº¿t Ä‘á»‹nh strategy dá»±a trÃªn relevance score.
        
        Returns:
            (is_relevant, strategy, warning_message, confidence)
        """
        top_score = details.get("top_score", 0)
        
        # === HIGH RELEVANCE: Search-based ===
        if relevance_score >= self.high_relevance_threshold and top_score >= self.min_top_score:
            return (
                True,
                "search",
                None,
                0.9
            )
        
        # === MEDIUM RELEVANCE: Hybrid approach ===
        elif relevance_score >= self.low_relevance_threshold:
            warning = (
                "âš ï¸ Query cÃ³ Ä‘á»™ liÃªn quan trung bÃ¬nh vá»›i tÃ i liá»‡u. "
                "Há»‡ thá»‘ng sáº½ káº¿t há»£p cáº£ ná»™i dung liÃªn quan vÃ  ná»™i dung Ä‘áº¡i diá»‡n."
            )
            return (
                True,
                "hybrid",
                warning,
                0.6
            )
        
        # === LOW RELEVANCE: Representative mode ===
        else:
            warning = (
                "âš ï¸ Query cÃ³ váº» KHÃ”NG liÃªn quan Ä‘áº¿n tÃ i liá»‡u nÃ y. "
                "Há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng sinh cÃ¢u há»i tá»« cÃ¡c pháº§n quan trá»ng cá»§a tÃ i liá»‡u "
                "thay vÃ¬ dá»±a vÃ o query cá»§a báº¡n."
            )
            return (
                False,
                "representative",
                warning,
                0.3
            )
    
    def quick_check(
        self,
        query: str,
        top_search_score: float
    ) -> bool:
        """
        Quick check nhanh chá»‰ dá»±a vÃ o top search score.
        DÃ¹ng khi khÃ´ng cÃ³ Ä‘á»§ dá»¯ liá»‡u cho phÃ¢n tÃ­ch Ä‘áº§y Ä‘á»§.
        
        Returns:
            True náº¿u cÃ³ váº» relevant, False náº¿u khÃ´ng
        """
        return top_search_score >= self.min_top_score


def create_relevance_detector(
    strict: bool = False
) -> QueryRelevanceDetector:
    """
    Factory function Ä‘á»ƒ táº¡o QueryRelevanceDetector.
    
    Args:
        strict: Náº¿u True, sá»­ dá»¥ng ngÆ°á»¡ng cháº·t cháº½ hÆ¡n
    
    Returns:
        QueryRelevanceDetector instance
    """
    if strict:
        return QueryRelevanceDetector(
            high_relevance_threshold=0.75,
            low_relevance_threshold=0.45,
            min_top_score=0.5,
            max_score_variance=0.12
        )
    else:
        return QueryRelevanceDetector(
            high_relevance_threshold=0.65,
            low_relevance_threshold=0.35,
            min_top_score=0.4,
            max_score_variance=0.15
        )
